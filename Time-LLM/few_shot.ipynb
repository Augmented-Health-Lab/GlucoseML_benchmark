{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ctfhMReUC8jo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769359768737,"user_tz":300,"elapsed":19532,"user":{"displayName":"PI DigitalSMD","userId":"16790990030515584217"}},"outputId":"3b20048b-4e7b-4c2e-9e8d-f23c21e875c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Working directory: /content/drive/Shareddrives/Baiying\n","/content/drive/Shareddrives/Baiying/Time-LLM\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","shared_path = '/content/drive/Shareddrives/Baiying'\n","os.chdir(shared_path)\n","print(\"Working directory:\", os.getcwd())\n","\n","%cd Time-LLM"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":112556,"status":"ok","timestamp":1769359881315,"user":{"displayName":"PI DigitalSMD","userId":"16790990030515584217"},"user_tz":300},"id":"RZZ7cL7s3gul","outputId":"94594aee-5d16-4b61-e3c5-59b14a1de7ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.2.2 (from -r requirements.txt (line 1))\n","  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting accelerate==0.28.0 (from -r requirements.txt (line 2))\n","  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n","Collecting einops==0.7.0 (from -r requirements.txt (line 3))\n","  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n","Collecting matplotlib==3.7.0 (from -r requirements.txt (line 4))\n","  Downloading matplotlib-3.7.0.tar.gz (36.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy<2.0,>=1.23.5 (from -r requirements.txt (line 5))\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas==1.5.3 (from -r requirements.txt (line 6))\n","  Downloading pandas-1.5.3.tar.gz (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting scikit_learn==1.2.2 (from -r requirements.txt (line 7))\n","  Downloading scikit-learn-1.2.2.tar.gz (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","source":["!python run_main.py \\\n","  --task_name long_term_forecast \\\n","  --is_training 1 \\\n","  --model TimeLLM \\\n","  --model_id Glucose_few_shot \\\n","  --model_comment \"GlucoseTrain_fewshot\" \\\n","  --llm_model GPT2 \\\n","  --llm_layers 4 \\\n","  --llm_dim 768 \\\n","  --data Glucose \\\n","  --root_path /content/drive/Shareddrives/Baiying/preprocessed_dataset/training_dataset/mixed \\\n","  --features S \\\n","  --target glucose \\\n","  --freq 5min \\\n","  --seq_len 144 \\\n","  --label_len 72 \\\n","  --pred_len 18 \\\n","  --enc_in 1 \\\n","  --dec_in 1 \\\n","  --c_out 1 \\\n","  --batch_size 16 \\\n","  --train_epochs 40 \\\n","  --learning_rate 5e-4 \\\n","  --num_workers 2 \\\n","  --stride 240 \\\n","  --patch_stride 3 \\\n","  --max_windows_per_epoch 30000 \\\n","  --des GlucoseTrain\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cm6fYnlSI21Y","outputId":"fca891a5-10d7-4cf2-beda-a464eadc2c3d","executionInfo":{"status":"ok","timestamp":1768785703743,"user_tz":300,"elapsed":1083293,"user":{"displayName":"PI DigitalSMD","userId":"16790990030515584217"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2026-01-19 01:04:07.620109: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2026-01-19 01:04:07.638109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1768784647.660057    5581 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1768784647.666735    5581 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1768784647.683553    5581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1768784647.683589    5581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1768784647.683592    5581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1768784647.683594    5581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-19 01:04:07.688590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[LLM] model=GPT2, layers=4, frozen=True\n","config.json: 100% 665/665 [00:00<00:00, 4.00MB/s]\n","pytorch_model.bin: 100% 548M/548M [00:03<00:00, 169MB/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 210kB/s]\n","vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.58MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 772kB/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 226kB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 36.7MB/s]\n","[LLM] model=GPT2, layers=4, frozen=True\n","99it [00:08, 15.06it/s]\titers: 100, epoch: 1 | loss: 0.0593851\n","\tspeed: 0.0875s/iter; left time: 1835.1055s\n","199it [00:14, 15.46it/s]\titers: 200, epoch: 1 | loss: 0.2249618\n","\tspeed: 0.0651s/iter; left time: 1359.8549s\n","299it [00:21, 15.36it/s]\titers: 300, epoch: 1 | loss: 0.0984811\n","\tspeed: 0.0649s/iter; left time: 1348.7284s\n","399it [00:27, 15.40it/s]\titers: 400, epoch: 1 | loss: 0.1983526\n","\tspeed: 0.0649s/iter; left time: 1342.8009s\n","499it [00:34, 15.23it/s]\titers: 500, epoch: 1 | loss: 0.2779139\n","\tspeed: 0.0654s/iter; left time: 1345.3547s\n","527it [00:36, 14.53it/s]\n","Epoch: 1 cost time: 36.26973342895508\n","71it [00:03, 17.76it/s]\n","Epoch: 1 | Train Loss: 0.2337489 | Val MAE: 0.2446095\n","lr = 0.0000200000\n","Updating learning rate to 1.9999999999999998e-05\n","99it [00:06, 16.25it/s]\titers: 100, epoch: 2 | loss: 0.0419962\n","\tspeed: 0.1331s/iter; left time: 2722.2478s\n","199it [00:12, 16.30it/s]\titers: 200, epoch: 2 | loss: 0.1492015\n","\tspeed: 0.0613s/iter; left time: 1247.5555s\n","299it [00:18, 15.59it/s]\titers: 300, epoch: 2 | loss: 0.0550485\n","\tspeed: 0.0625s/iter; left time: 1265.5966s\n","399it [00:24, 16.48it/s]\titers: 400, epoch: 2 | loss: 0.1425821\n","\tspeed: 0.0614s/iter; left time: 1236.8863s\n","499it [00:30, 16.40it/s]\titers: 500, epoch: 2 | loss: 0.2091202\n","\tspeed: 0.0614s/iter; left time: 1232.2148s\n","527it [00:32, 16.20it/s]\n","Epoch: 2 cost time: 32.53400731086731\n","71it [00:02, 26.12it/s]\n","Epoch: 2 | Train Loss: 0.1569963 | Val MAE: 0.2041619\n","Updating learning rate to 9.999999999999999e-06\n","99it [00:06, 16.28it/s]\titers: 100, epoch: 3 | loss: 0.0455506\n","\tspeed: 0.1172s/iter; left time: 2335.8489s\n","199it [00:12, 16.37it/s]\titers: 200, epoch: 3 | loss: 0.1390629\n","\tspeed: 0.0610s/iter; left time: 1209.0309s\n","299it [00:18, 16.20it/s]\titers: 300, epoch: 3 | loss: 0.0443436\n","\tspeed: 0.0619s/iter; left time: 1221.4340s\n","399it [00:24, 16.43it/s]\titers: 400, epoch: 3 | loss: 0.1263531\n","\tspeed: 0.0610s/iter; left time: 1197.4989s\n","499it [00:30, 16.34it/s]\titers: 500, epoch: 3 | loss: 0.1945543\n","\tspeed: 0.0613s/iter; left time: 1196.2482s\n","527it [00:32, 16.26it/s]\n","Epoch: 3 cost time: 32.4167046546936\n","71it [00:02, 26.36it/s]\n","Epoch: 3 | Train Loss: 0.1328683 | Val MAE: 0.1972525\n","Updating learning rate to 4.9999999999999996e-06\n","99it [00:06, 16.12it/s]\titers: 100, epoch: 4 | loss: 0.0490768\n","\tspeed: 0.1186s/iter; left time: 2301.7421s\n","199it [00:12, 16.19it/s]\titers: 200, epoch: 4 | loss: 0.1495319\n","\tspeed: 0.0610s/iter; left time: 1177.4597s\n","299it [00:18, 15.82it/s]\titers: 300, epoch: 4 | loss: 0.0480459\n","\tspeed: 0.0618s/iter; left time: 1187.0976s\n","399it [00:24, 16.54it/s]\titers: 400, epoch: 4 | loss: 0.1176755\n","\tspeed: 0.0611s/iter; left time: 1167.3713s\n","499it [00:30, 15.87it/s]\titers: 500, epoch: 4 | loss: 0.1833295\n","\tspeed: 0.0615s/iter; left time: 1168.8426s\n","527it [00:32, 16.25it/s]\n","Epoch: 4 cost time: 32.437960147857666\n","71it [00:02, 26.15it/s]\n","Epoch: 4 | Train Loss: 0.1264531 | Val MAE: 0.1913813\n","Updating learning rate to 2.4999999999999998e-06\n","99it [00:06, 16.14it/s]\titers: 100, epoch: 5 | loss: 0.0479287\n","\tspeed: 0.1178s/iter; left time: 2223.8751s\n","199it [00:12, 16.51it/s]\titers: 200, epoch: 5 | loss: 0.1150285\n","\tspeed: 0.0612s/iter; left time: 1149.0783s\n","299it [00:18, 16.00it/s]\titers: 300, epoch: 5 | loss: 0.0445273\n","\tspeed: 0.0617s/iter; left time: 1151.6093s\n","399it [00:24, 16.21it/s]\titers: 400, epoch: 5 | loss: 0.1010237\n","\tspeed: 0.0619s/iter; left time: 1149.1233s\n","499it [00:30, 16.07it/s]\titers: 500, epoch: 5 | loss: 0.1974380\n","\tspeed: 0.0619s/iter; left time: 1142.7298s\n","527it [00:32, 16.18it/s]\n","Epoch: 5 cost time: 32.56170034408569\n","71it [00:02, 26.24it/s]\n","Epoch: 5 | Train Loss: 0.1228538 | Val MAE: 0.1881308\n","Updating learning rate to 1.2499999999999999e-06\n","99it [00:06, 15.64it/s]\titers: 100, epoch: 6 | loss: 0.0550589\n","\tspeed: 0.1186s/iter; left time: 2176.4466s\n","199it [00:12, 16.15it/s]\titers: 200, epoch: 6 | loss: 0.1231000\n","\tspeed: 0.0623s/iter; left time: 1136.4466s\n","299it [00:18, 16.24it/s]\titers: 300, epoch: 6 | loss: 0.0426670\n","\tspeed: 0.0615s/iter; left time: 1116.2460s\n","399it [00:24, 16.27it/s]\titers: 400, epoch: 6 | loss: 0.1128748\n","\tspeed: 0.0617s/iter; left time: 1112.9714s\n","499it [00:30, 16.28it/s]\titers: 500, epoch: 6 | loss: 0.1763486\n","\tspeed: 0.0611s/iter; left time: 1096.7043s\n","527it [00:32, 16.14it/s]\n","Epoch: 6 cost time: 32.6587438583374\n","71it [00:02, 26.08it/s]\n","Epoch: 6 | Train Loss: 0.1237088 | Val MAE: 0.1882545\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 6.249999999999999e-07\n","99it [00:06, 16.38it/s]\titers: 100, epoch: 7 | loss: 0.0481357\n","\tspeed: 0.1060s/iter; left time: 1887.9934s\n","199it [00:12, 16.20it/s]\titers: 200, epoch: 7 | loss: 0.1467408\n","\tspeed: 0.0612s/iter; left time: 1084.5244s\n","299it [00:18, 16.28it/s]\titers: 300, epoch: 7 | loss: 0.0454998\n","\tspeed: 0.0611s/iter; left time: 1076.4332s\n","399it [00:24, 16.45it/s]\titers: 400, epoch: 7 | loss: 0.1201798\n","\tspeed: 0.0610s/iter; left time: 1068.8677s\n","499it [00:30, 16.32it/s]\titers: 500, epoch: 7 | loss: 0.1803551\n","\tspeed: 0.0612s/iter; left time: 1065.5699s\n","527it [00:32, 16.30it/s]\n","Epoch: 7 cost time: 32.32680559158325\n","71it [00:02, 26.39it/s]\n","Epoch: 7 | Train Loss: 0.1226326 | Val MAE: 0.1886001\n","Updating learning rate to 3.1249999999999997e-07\n","99it [00:06, 16.28it/s]\titers: 100, epoch: 8 | loss: 0.0483563\n","\tspeed: 0.1175s/iter; left time: 2031.6539s\n","199it [00:12, 16.07it/s]\titers: 200, epoch: 8 | loss: 0.1343766\n","\tspeed: 0.0619s/iter; left time: 1063.8153s\n","299it [00:18, 16.21it/s]\titers: 300, epoch: 8 | loss: 0.0497331\n","\tspeed: 0.0619s/iter; left time: 1058.6727s\n","399it [00:24, 16.08it/s]\titers: 400, epoch: 8 | loss: 0.1171446\n","\tspeed: 0.0620s/iter; left time: 1053.5637s\n","499it [00:30, 16.05it/s]\titers: 500, epoch: 8 | loss: 0.1590519\n","\tspeed: 0.0623s/iter; left time: 1052.1155s\n","527it [00:32, 16.09it/s]\n","Epoch: 8 cost time: 32.751033544540405\n","71it [00:02, 26.11it/s]\n","Epoch: 8 | Train Loss: 0.1212889 | Val MAE: 0.1886057\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 1.5624999999999999e-07\n","99it [00:06, 16.26it/s]\titers: 100, epoch: 9 | loss: 0.0507396\n","\tspeed: 0.1064s/iter; left time: 1784.1631s\n","199it [00:12, 16.18it/s]\titers: 200, epoch: 9 | loss: 0.1190751\n","\tspeed: 0.0621s/iter; left time: 1034.2476s\n","299it [00:18, 16.31it/s]\titers: 300, epoch: 9 | loss: 0.0467116\n","\tspeed: 0.0611s/iter; left time: 1012.9261s\n","399it [00:24, 16.48it/s]\titers: 400, epoch: 9 | loss: 0.0945485\n","\tspeed: 0.0612s/iter; left time: 1007.7882s\n","499it [00:30, 16.47it/s]\titers: 500, epoch: 9 | loss: 0.1772422\n","\tspeed: 0.0608s/iter; left time: 995.2966s\n","527it [00:32, 16.26it/s]\n","Epoch: 9 cost time: 32.40818929672241\n","71it [00:02, 25.70it/s]\n","Epoch: 9 | Train Loss: 0.1216075 | Val MAE: 0.1882321\n","Updating learning rate to 7.812499999999999e-08\n","99it [00:06, 16.23it/s]\titers: 100, epoch: 10 | loss: 0.0607549\n","\tspeed: 0.1181s/iter; left time: 1916.9802s\n","199it [00:12, 16.46it/s]\titers: 200, epoch: 10 | loss: 0.1237921\n","\tspeed: 0.0616s/iter; left time: 993.2996s\n","299it [00:18, 16.35it/s]\titers: 300, epoch: 10 | loss: 0.0438317\n","\tspeed: 0.0613s/iter; left time: 982.8037s\n","399it [00:24, 16.44it/s]\titers: 400, epoch: 10 | loss: 0.1082542\n","\tspeed: 0.0615s/iter; left time: 979.8383s\n","499it [00:30, 16.38it/s]\titers: 500, epoch: 10 | loss: 0.1796869\n","\tspeed: 0.0611s/iter; left time: 967.4648s\n","527it [00:32, 16.23it/s]\n","Epoch: 10 cost time: 32.4626362323761\n","71it [00:02, 25.74it/s]\n","Epoch: 10 | Train Loss: 0.1224974 | Val MAE: 0.1879404\n","Updating learning rate to 3.9062499999999997e-08\n","99it [00:06, 16.42it/s]\titers: 100, epoch: 11 | loss: 0.0406899\n","\tspeed: 0.1174s/iter; left time: 1844.3498s\n","199it [00:12, 16.47it/s]\titers: 200, epoch: 11 | loss: 0.1132753\n","\tspeed: 0.0614s/iter; left time: 958.1215s\n","299it [00:18, 16.42it/s]\titers: 300, epoch: 11 | loss: 0.0486937\n","\tspeed: 0.0610s/iter; left time: 946.8429s\n","399it [00:24, 16.11it/s]\titers: 400, epoch: 11 | loss: 0.1164051\n","\tspeed: 0.0615s/iter; left time: 948.3610s\n","499it [00:30, 16.28it/s]\titers: 500, epoch: 11 | loss: 0.1748842\n","\tspeed: 0.0615s/iter; left time: 942.1622s\n","527it [00:32, 16.25it/s]\n","Epoch: 11 cost time: 32.43904185295105\n","71it [00:02, 25.76it/s]\n","Epoch: 11 | Train Loss: 0.1227665 | Val MAE: 0.1881180\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 1.9531249999999998e-08\n","99it [00:06, 16.33it/s]\titers: 100, epoch: 12 | loss: 0.0498380\n","\tspeed: 0.1061s/iter; left time: 1611.0250s\n","199it [00:12, 16.34it/s]\titers: 200, epoch: 12 | loss: 0.1146595\n","\tspeed: 0.0612s/iter; left time: 922.7831s\n","299it [00:18, 16.23it/s]\titers: 300, epoch: 12 | loss: 0.0516390\n","\tspeed: 0.0612s/iter; left time: 917.2266s\n","399it [00:24, 16.41it/s]\titers: 400, epoch: 12 | loss: 0.1117375\n","\tspeed: 0.0611s/iter; left time: 909.5184s\n","499it [00:30, 16.41it/s]\titers: 500, epoch: 12 | loss: 0.1748785\n","\tspeed: 0.0616s/iter; left time: 911.0746s\n","527it [00:32, 16.29it/s]\n","Epoch: 12 cost time: 32.351945638656616\n","71it [00:02, 26.05it/s]\n","Epoch: 12 | Train Loss: 0.1216777 | Val MAE: 0.1872562\n","Updating learning rate to 9.765624999999999e-09\n","99it [00:06, 15.96it/s]\titers: 100, epoch: 13 | loss: 0.0473087\n","\tspeed: 0.1175s/iter; left time: 1722.0976s\n","199it [00:12, 16.37it/s]\titers: 200, epoch: 13 | loss: 0.1091100\n","\tspeed: 0.0614s/iter; left time: 894.3556s\n","299it [00:18, 16.33it/s]\titers: 300, epoch: 13 | loss: 0.0492766\n","\tspeed: 0.0611s/iter; left time: 883.8883s\n","399it [00:24, 16.19it/s]\titers: 400, epoch: 13 | loss: 0.1198394\n","\tspeed: 0.0618s/iter; left time: 887.2410s\n","499it [00:30, 16.42it/s]\titers: 500, epoch: 13 | loss: 0.1621837\n","\tspeed: 0.0613s/iter; left time: 874.3038s\n","527it [00:32, 16.23it/s]\n","Epoch: 13 cost time: 32.470988512039185\n","71it [00:02, 26.12it/s]\n","Epoch: 13 | Train Loss: 0.1222096 | Val MAE: 0.1881203\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 4.8828124999999996e-09\n","99it [00:06, 16.38it/s]\titers: 100, epoch: 14 | loss: 0.0461244\n","\tspeed: 0.1056s/iter; left time: 1492.1065s\n","199it [00:12, 16.39it/s]\titers: 200, epoch: 14 | loss: 0.1387238\n","\tspeed: 0.0609s/iter; left time: 855.0495s\n","299it [00:18, 16.36it/s]\titers: 300, epoch: 14 | loss: 0.0482400\n","\tspeed: 0.0609s/iter; left time: 848.1627s\n","399it [00:24, 16.43it/s]\titers: 400, epoch: 14 | loss: 0.1176316\n","\tspeed: 0.0610s/iter; left time: 843.3252s\n","499it [00:30, 16.42it/s]\titers: 500, epoch: 14 | loss: 0.1573733\n","\tspeed: 0.0611s/iter; left time: 838.2285s\n","527it [00:32, 16.36it/s]\n","Epoch: 14 cost time: 32.22165870666504\n","71it [00:02, 26.28it/s]\n","Epoch: 14 | Train Loss: 0.1218645 | Val MAE: 0.1875814\n","EarlyStopping counter: 2 out of 10\n","Updating learning rate to 2.4414062499999998e-09\n","99it [00:06, 16.33it/s]\titers: 100, epoch: 15 | loss: 0.0465227\n","\tspeed: 0.1057s/iter; left time: 1438.2085s\n","199it [00:12, 14.77it/s]\titers: 200, epoch: 15 | loss: 0.1250869\n","\tspeed: 0.0661s/iter; left time: 893.0465s\n","299it [00:18, 16.40it/s]\titers: 300, epoch: 15 | loss: 0.0494566\n","\tspeed: 0.0613s/iter; left time: 821.8475s\n","399it [00:25, 16.31it/s]\titers: 400, epoch: 15 | loss: 0.1019074\n","\tspeed: 0.0615s/iter; left time: 817.9790s\n","499it [00:31, 16.28it/s]\titers: 500, epoch: 15 | loss: 0.1817084\n","\tspeed: 0.0614s/iter; left time: 810.6496s\n","527it [00:32, 16.00it/s]\n","Epoch: 15 cost time: 32.92985224723816\n","71it [00:02, 25.84it/s]\n","Epoch: 15 | Train Loss: 0.1219242 | Val MAE: 0.1875757\n","EarlyStopping counter: 3 out of 10\n","Updating learning rate to 1.2207031249999999e-09\n","99it [00:06, 16.17it/s]\titers: 100, epoch: 16 | loss: 0.0473014\n","\tspeed: 0.1070s/iter; left time: 1399.6178s\n","199it [00:12, 16.24it/s]\titers: 200, epoch: 16 | loss: 0.1261439\n","\tspeed: 0.0619s/iter; left time: 803.4535s\n","299it [00:18, 16.19it/s]\titers: 300, epoch: 16 | loss: 0.0447437\n","\tspeed: 0.0619s/iter; left time: 797.4421s\n","399it [00:24, 16.36it/s]\titers: 400, epoch: 16 | loss: 0.0990538\n","\tspeed: 0.0616s/iter; left time: 787.2379s\n","499it [00:30, 16.21it/s]\titers: 500, epoch: 16 | loss: 0.1831844\n","\tspeed: 0.0615s/iter; left time: 780.1608s\n","527it [00:32, 16.14it/s]\n","Epoch: 16 cost time: 32.65887928009033\n","71it [00:02, 26.01it/s]\n","Epoch: 16 | Train Loss: 0.1200593 | Val MAE: 0.1876114\n","EarlyStopping counter: 4 out of 10\n","Updating learning rate to 6.103515624999999e-10\n","99it [00:06, 16.32it/s]\titers: 100, epoch: 17 | loss: 0.0501054\n","\tspeed: 0.1066s/iter; left time: 1337.9949s\n","199it [00:12, 16.38it/s]\titers: 200, epoch: 17 | loss: 0.1258417\n","\tspeed: 0.0613s/iter; left time: 762.7716s\n","299it [00:18, 16.28it/s]\titers: 300, epoch: 17 | loss: 0.0417760\n","\tspeed: 0.0613s/iter; left time: 757.5125s\n","399it [00:24, 16.32it/s]\titers: 400, epoch: 17 | loss: 0.0989029\n","\tspeed: 0.0617s/iter; left time: 755.4729s\n","499it [00:30, 16.34it/s]\titers: 500, epoch: 17 | loss: 0.1850251\n","\tspeed: 0.0613s/iter; left time: 744.4361s\n","527it [00:32, 16.23it/s]\n","Epoch: 17 cost time: 32.47127556800842\n","71it [00:02, 25.92it/s]\n","Epoch: 17 | Train Loss: 0.1220276 | Val MAE: 0.1872961\n","Updating learning rate to 3.0517578124999997e-10\n","99it [00:06, 16.24it/s]\titers: 100, epoch: 18 | loss: 0.0522320\n","\tspeed: 0.1178s/iter; left time: 1415.5923s\n","199it [00:12, 16.14it/s]\titers: 200, epoch: 18 | loss: 0.1124730\n","\tspeed: 0.0618s/iter; left time: 736.7203s\n","299it [00:18, 16.30it/s]\titers: 300, epoch: 18 | loss: 0.0533362\n","\tspeed: 0.0620s/iter; left time: 732.5648s\n","399it [00:24, 16.14it/s]\titers: 400, epoch: 18 | loss: 0.1054740\n","\tspeed: 0.0616s/iter; left time: 722.5923s\n","499it [00:30, 16.37it/s]\titers: 500, epoch: 18 | loss: 0.1822079\n","\tspeed: 0.0616s/iter; left time: 715.6388s\n","527it [00:32, 16.16it/s]\n","Epoch: 18 cost time: 32.62180829048157\n","71it [00:02, 26.10it/s]\n","Epoch: 18 | Train Loss: 0.1216225 | Val MAE: 0.1878770\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 1.5258789062499999e-10\n","99it [00:06, 16.19it/s]\titers: 100, epoch: 19 | loss: 0.0520767\n","\tspeed: 0.1064s/iter; left time: 1223.4455s\n","199it [00:12, 16.11it/s]\titers: 200, epoch: 19 | loss: 0.1368900\n","\tspeed: 0.0619s/iter; left time: 705.6968s\n","299it [00:18, 16.25it/s]\titers: 300, epoch: 19 | loss: 0.0453210\n","\tspeed: 0.0621s/iter; left time: 701.3502s\n","399it [00:24, 16.16it/s]\titers: 400, epoch: 19 | loss: 0.1053725\n","\tspeed: 0.0617s/iter; left time: 690.4990s\n","499it [00:30, 16.36it/s]\titers: 500, epoch: 19 | loss: 0.1636149\n","\tspeed: 0.0617s/iter; left time: 684.6854s\n","527it [00:32, 16.14it/s]\n","Epoch: 19 cost time: 32.66012001037598\n","71it [00:02, 26.02it/s]\n","Epoch: 19 | Train Loss: 0.1210326 | Val MAE: 0.1879540\n","EarlyStopping counter: 2 out of 10\n","Updating learning rate to 7.629394531249999e-11\n","99it [00:06, 16.24it/s]\titers: 100, epoch: 20 | loss: 0.0452703\n","\tspeed: 0.1065s/iter; left time: 1168.5767s\n","199it [00:12, 16.12it/s]\titers: 200, epoch: 20 | loss: 0.1189459\n","\tspeed: 0.0617s/iter; left time: 670.3718s\n","299it [00:18, 16.14it/s]\titers: 300, epoch: 20 | loss: 0.0483921\n","\tspeed: 0.0618s/iter; left time: 665.1100s\n","399it [00:24, 16.41it/s]\titers: 400, epoch: 20 | loss: 0.0997671\n","\tspeed: 0.0617s/iter; left time: 657.9407s\n","499it [00:30, 16.25it/s]\titers: 500, epoch: 20 | loss: 0.1733028\n","\tspeed: 0.0614s/iter; left time: 648.7878s\n","527it [00:32, 16.17it/s]\n","Epoch: 20 cost time: 32.582011461257935\n","71it [00:02, 25.51it/s]\n","Epoch: 20 | Train Loss: 0.1218536 | Val MAE: 0.1877638\n","EarlyStopping counter: 3 out of 10\n","Updating learning rate to 3.8146972656249997e-11\n","99it [00:06, 16.27it/s]\titers: 100, epoch: 21 | loss: 0.0495379\n","\tspeed: 0.1070s/iter; left time: 1117.1524s\n","199it [00:12, 16.16it/s]\titers: 200, epoch: 21 | loss: 0.1028908\n","\tspeed: 0.0618s/iter; left time: 638.7501s\n","299it [00:18, 16.17it/s]\titers: 300, epoch: 21 | loss: 0.0509896\n","\tspeed: 0.0616s/iter; left time: 630.7469s\n","399it [00:24, 16.06it/s]\titers: 400, epoch: 21 | loss: 0.1137725\n","\tspeed: 0.0620s/iter; left time: 628.6072s\n","499it [00:30, 16.16it/s]\titers: 500, epoch: 21 | loss: 0.1862500\n","\tspeed: 0.0616s/iter; left time: 618.6562s\n","527it [00:32, 16.15it/s]\n","Epoch: 21 cost time: 32.62652134895325\n","71it [00:02, 25.64it/s]\n","Epoch: 21 | Train Loss: 0.1216614 | Val MAE: 0.1875304\n","EarlyStopping counter: 4 out of 10\n","Updating learning rate to 1.9073486328124998e-11\n","99it [00:06, 16.44it/s]\titers: 100, epoch: 22 | loss: 0.0427583\n","\tspeed: 0.1068s/iter; left time: 1058.7428s\n","199it [00:12, 16.12it/s]\titers: 200, epoch: 22 | loss: 0.1279911\n","\tspeed: 0.0615s/iter; left time: 603.7957s\n","299it [00:18, 16.37it/s]\titers: 300, epoch: 22 | loss: 0.0515956\n","\tspeed: 0.0614s/iter; left time: 596.8608s\n","399it [00:24, 16.28it/s]\titers: 400, epoch: 22 | loss: 0.1179325\n","\tspeed: 0.0623s/iter; left time: 599.0403s\n","499it [00:30, 16.29it/s]\titers: 500, epoch: 22 | loss: 0.1623280\n","\tspeed: 0.0615s/iter; left time: 585.0329s\n","527it [00:32, 16.17it/s]\n","Epoch: 22 cost time: 32.5942702293396\n","71it [00:02, 25.44it/s]\n","Epoch: 22 | Train Loss: 0.1226469 | Val MAE: 0.1876758\n","EarlyStopping counter: 5 out of 10\n","Updating learning rate to 9.536743164062499e-12\n","99it [00:06, 16.19it/s]\titers: 100, epoch: 23 | loss: 0.0486194\n","\tspeed: 0.1078s/iter; left time: 1011.5549s\n","199it [00:12, 16.12it/s]\titers: 200, epoch: 23 | loss: 0.1141083\n","\tspeed: 0.0619s/iter; left time: 575.0590s\n","299it [00:18, 16.23it/s]\titers: 300, epoch: 23 | loss: 0.0557355\n","\tspeed: 0.0618s/iter; left time: 568.0114s\n","399it [00:24, 16.09it/s]\titers: 400, epoch: 23 | loss: 0.1077123\n","\tspeed: 0.0618s/iter; left time: 561.8441s\n","499it [00:30, 16.27it/s]\titers: 500, epoch: 23 | loss: 0.1732932\n","\tspeed: 0.0618s/iter; left time: 555.3459s\n","527it [00:32, 16.12it/s]\n","Epoch: 23 cost time: 32.697787046432495\n","71it [00:02, 25.20it/s]\n","Epoch: 23 | Train Loss: 0.1220113 | Val MAE: 0.1879903\n","EarlyStopping counter: 6 out of 10\n","Updating learning rate to 4.7683715820312496e-12\n","99it [00:06, 15.99it/s]\titers: 100, epoch: 24 | loss: 0.0403964\n","\tspeed: 0.1076s/iter; left time: 953.4728s\n","199it [00:12, 16.13it/s]\titers: 200, epoch: 24 | loss: 0.1228059\n","\tspeed: 0.0620s/iter; left time: 543.4965s\n","299it [00:18, 16.15it/s]\titers: 300, epoch: 24 | loss: 0.0466433\n","\tspeed: 0.0621s/iter; left time: 538.0711s\n","399it [00:24, 16.20it/s]\titers: 400, epoch: 24 | loss: 0.1040736\n","\tspeed: 0.0622s/iter; left time: 532.3776s\n","499it [00:31, 16.08it/s]\titers: 500, epoch: 24 | loss: 0.1669281\n","\tspeed: 0.0621s/iter; left time: 525.4441s\n","527it [00:32, 16.06it/s]\n","Epoch: 24 cost time: 32.81073808670044\n","71it [00:02, 25.43it/s]\n","Epoch: 24 | Train Loss: 0.1208246 | Val MAE: 0.1877135\n","EarlyStopping counter: 7 out of 10\n","Updating learning rate to 2.3841857910156248e-12\n","99it [00:06, 16.00it/s]\titers: 100, epoch: 25 | loss: 0.0498454\n","\tspeed: 0.1083s/iter; left time: 902.7928s\n","199it [00:12, 16.10it/s]\titers: 200, epoch: 25 | loss: 0.1325618\n","\tspeed: 0.0625s/iter; left time: 514.2452s\n","299it [00:18, 16.32it/s]\titers: 300, epoch: 25 | loss: 0.0525750\n","\tspeed: 0.0617s/iter; left time: 501.6883s\n","399it [00:24, 16.15it/s]\titers: 400, epoch: 25 | loss: 0.1114568\n","\tspeed: 0.0617s/iter; left time: 495.4295s\n","499it [00:31, 16.17it/s]\titers: 500, epoch: 25 | loss: 0.1956636\n","\tspeed: 0.0616s/iter; left time: 488.5774s\n","527it [00:32, 16.08it/s]\n","Epoch: 25 cost time: 32.77342939376831\n","71it [00:02, 25.83it/s]\n","Epoch: 25 | Train Loss: 0.1212491 | Val MAE: 0.1876211\n","EarlyStopping counter: 8 out of 10\n","Updating learning rate to 1.1920928955078124e-12\n","99it [00:06, 16.32it/s]\titers: 100, epoch: 26 | loss: 0.0459014\n","\tspeed: 0.1069s/iter; left time: 834.3614s\n","199it [00:12, 16.16it/s]\titers: 200, epoch: 26 | loss: 0.1258962\n","\tspeed: 0.0618s/iter; left time: 476.2000s\n","299it [00:18, 16.11it/s]\titers: 300, epoch: 26 | loss: 0.0510435\n","\tspeed: 0.0620s/iter; left time: 471.9313s\n","399it [00:24, 16.16it/s]\titers: 400, epoch: 26 | loss: 0.1149765\n","\tspeed: 0.0624s/iter; left time: 468.6172s\n","499it [00:31, 16.14it/s]\titers: 500, epoch: 26 | loss: 0.1646455\n","\tspeed: 0.0622s/iter; left time: 460.8759s\n","527it [00:32, 16.07it/s]\n","Epoch: 26 cost time: 32.78468728065491\n","71it [00:02, 26.00it/s]\n","Epoch: 26 | Train Loss: 0.1210048 | Val MAE: 0.1877253\n","EarlyStopping counter: 9 out of 10\n","Updating learning rate to 5.960464477539062e-13\n","99it [00:06, 16.18it/s]\titers: 100, epoch: 27 | loss: 0.0508333\n","\tspeed: 0.1071s/iter; left time: 779.4763s\n","199it [00:12, 16.23it/s]\titers: 200, epoch: 27 | loss: 0.1183515\n","\tspeed: 0.0618s/iter; left time: 443.8137s\n","299it [00:18, 16.12it/s]\titers: 300, epoch: 27 | loss: 0.0504864\n","\tspeed: 0.0622s/iter; left time: 440.4036s\n","399it [00:24, 16.16it/s]\titers: 400, epoch: 27 | loss: 0.1057078\n","\tspeed: 0.0618s/iter; left time: 431.2174s\n","499it [00:31, 16.15it/s]\titers: 500, epoch: 27 | loss: 0.1618936\n","\tspeed: 0.0623s/iter; left time: 428.5684s\n","527it [00:32, 16.07it/s]\n","Epoch: 27 cost time: 32.793843269348145\n","71it [00:02, 25.53it/s]\n","Epoch: 27 | Train Loss: 0.1221067 | Val MAE: 0.1875041\n","EarlyStopping counter: 10 out of 10\n","Early stopping\n","[W119 01:21:42.752677847 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"]}]},{"cell_type":"code","source":["!python run_main.py \\\n","  --task_name long_term_forecast \\\n","  --is_training 0 \\\n","  --model TimeLLM \\\n","  --model_id Glucose_few_shot \\\n","  --model_comment \"GlucoseTrain_fewshot\" \\\n","  --llm_model GPT2 \\\n","  --llm_layers 4 \\\n","  --llm_dim 768 \\\n","  --data Glucose \\\n","  --test_root_path /content/drive/Shareddrives/Baiying/preprocessed_dataset/test_dataset/mixed \\\n","  --features S \\\n","  --target glucose \\\n","  --freq 5min \\\n","  --seq_len 144 \\\n","  --label_len 72 \\\n","  --pred_len 18 \\\n","  --enc_in 1 \\\n","  --dec_in 1 \\\n","  --c_out 1 \\\n","  --batch_size 16 \\\n","  --num_workers 2 \\\n","  --stride 1 \\\n","  --patch_stride 3 \\\n","  --des GlucoseTrain\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kapz9LTX7_M","executionInfo":{"status":"ok","timestamp":1768787789986,"user_tz":300,"elapsed":1386864,"user":{"displayName":"PI DigitalSMD","userId":"16790990030515584217"}},"outputId":"e4849088-2eb3-4d7d-f0a1-39e40c279cc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2026-01-19 01:33:29.411562: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2026-01-19 01:33:29.429292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1768786409.450573   15018 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1768786409.457195   15018 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1768786409.473687   15018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1768786409.473719   15018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1768786409.473723   15018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1768786409.473727   15018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-19 01:33:29.478716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[LLM] model=GPT2, layers=4, frozen=True\n","pytorch_model.bin: 100% 548M/548M [00:02<00:00, 250MB/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 182kB/s]\n","vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.22MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 2.16MB/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 216kB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 2.36MB/s]\n","[LLM] model=GPT2, layers=4, frozen=True\n",">>> Running TEST-ONLY evaluation\n",">>> Loading checkpoint from ./checkpoints/long_term_forecast_Glucose_few_shot_TimeLLM_Glucose_ftS_sl144_ll72_pl18_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_GlucoseTrain_0-GlucoseTrain_fewshot/checkpoint\n","32140it [22:11, 24.14it/s]\n","[TEST] Saved per-subject overall metrics -> ./checkpoints/long_term_forecast_Glucose_few_shot_TimeLLM_Glucose_ftS_sl144_ll72_pl18_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_GlucoseTrain_0-GlucoseTrain_fewshot/test_metrics/per_subject_overall.csv\n","[TEST] Saved per-subject horizon metrics -> ./checkpoints/long_term_forecast_Glucose_few_shot_TimeLLM_Glucose_ftS_sl144_ll72_pl18_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_GlucoseTrain_0-GlucoseTrain_fewshot/test_metrics/per_subject_horizons.csv\n","[TEST] MSE: 0.099271, MAE: 0.208168\n","[W119 01:56:29.191301530 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"]}]},{"cell_type":"code","source":["# Ohiot1dm\n","!python run_main.py \\\n","  --task_name long_term_forecast \\\n","  --is_training 0 \\\n","  --model TimeLLM \\\n","  --model_id Glucose_few_shot \\\n","  --model_comment \"GlucoseTrain_fewshot\" \\\n","  --llm_model GPT2 \\\n","  --llm_layers 4 \\\n","  --llm_dim 768 \\\n","  --data Glucose \\\n","  --test_root_path /content/drive/Shareddrives/Baiying/preprocessed_dataset/test_dataset/controlled_datasets/8_DiaTrend \\\n","  --features S \\\n","  --target glucose \\\n","  --freq 5min \\\n","  --seq_len 144 \\\n","  --label_len 72 \\\n","  --pred_len 18 \\\n","  --enc_in 1 \\\n","  --dec_in 1 \\\n","  --c_out 1 \\\n","  --batch_size 16 \\\n","  --num_workers 2 \\\n","  --stride 1 \\\n","  --patch_stride 3 \\\n","  --des GlucoseTrain\n"],"metadata":{"id":"uDS2rsrBaef2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769367787511,"user_tz":300,"elapsed":3201996,"user":{"displayName":"PI DigitalSMD","userId":"16790990030515584217"}},"outputId":"057ce953-e2d7-41c9-d2cb-af3010a20727"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2026-01-25 18:09:51.788987: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2026-01-25 18:09:51.807002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769364591.828449   23093 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769364591.834950   23093 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769364591.851594   23093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769364591.851621   23093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769364591.851623   23093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769364591.851626   23093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-25 18:09:51.856416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[LLM] model=GPT2, layers=4, frozen=True\n","pytorch_model.bin: 100% 548M/548M [00:02<00:00, 246MB/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 193kB/s]\n","vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.38MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 10.6MB/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 180kB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 18.1MB/s]\n","[LLM] model=GPT2, layers=4, frozen=True\n",">>> Running TEST-ONLY evaluation\n",">>> Loading checkpoint from ./checkpoints/long_term_forecast_Glucose_few_shot_TimeLLM_Glucose_ftS_sl144_ll72_pl18_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_GlucoseTrain_0-GlucoseTrain_fewshot/checkpoint\n","75721it [52:03, 24.25it/s]\n","[TEST] Saved per-subject overall metrics -> ./checkpoints/long_term_forecast_Glucose_few_shot_TimeLLM_Glucose_ftS_sl144_ll72_pl18_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_GlucoseTrain_0-GlucoseTrain_fewshot/test_metrics/per_subject_overall.csv\n","[TEST] Saved per-subject horizon metrics -> ./checkpoints/long_term_forecast_Glucose_few_shot_TimeLLM_Glucose_ftS_sl144_ll72_pl18_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_GlucoseTrain_0-GlucoseTrain_fewshot/test_metrics/per_subject_horizons.csv\n","[TEST] MSE: 0.210247, MAE: 0.331919\n","[W125 19:03:07.357755968 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"sWYS6XonEhGz"},"execution_count":null,"outputs":[]}]}